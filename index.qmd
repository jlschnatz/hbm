---
title: "Grundlagen Hierarchischer Bayesianischer-Modelle"
subtitle: "Formalisierung induktiver Biases am Beispiel des Kategorienlernens"
author: "Jan Luca Schnatz <br> Martikelnummer: 7518898"
format:
  revealjs:
    slide-number: false
    css: assets/styles.css
    chalkboard: false
    center-title-slide: true
    progress: true
    transition: fade
    embed-resources: true
    self-contained-math: true
    smaller: true
    margin: 0.2
    center: true
    auto-stretch: false
    touch: false
    auto-animate: true
    theme: [simple, assets/styles.scss]
    pointer:
      key: p
      pointerSize: 14
      color: "#000000"
lightbox: false
code-block-border-left: true
mathfont: Tex Gyre Pagella
html-math-method: mathjax
bibliography: assets/references.bib
csl: assets/apa7.csl
include-after-body: 
  - file: assets/revealjs-tabset.html
revealjs-plugins:
  - pointer
filters: 
  - assets/linebreaks.lua
---

```{r load-packages}
library(tidyverse)
library(kableExtra)
options(knitr.kable.NA = '?')
```

# Hierarchisches Beta-Binomiales Modell {.dark-theme}

## Einführendes Beispiel

Wiederholtes Ziehen von schwarzen und weißen Murmeln aus verschiedenen Murmelbeuteln

:::::::::::: marble-grid
::::::::::: {layout-ncol="4"}
::: fragment
![][1]
:::

::: fragment
![][2]
:::

::: fragment
![][3]
:::

::: fragment
![][4]
:::

::: fragment
![][5]
:::

::: fragment
![][6]
:::

::: fragment
![][7]
:::

::: fragment
![][8]
:::
:::::::::::
::::::::::::

::: fragment
Welche Farbe ist für die nächste Murmel im achten Murmelbeutel am wahrscheinlichsten?
:::

## Intuition {auto-animate="true"}

- Eine einzelne schwarze Murmel liefert wenig Information über zukünftige Murmeln.
- Durch Vorwissen über viele Murmelbeutel (meist schwarz oder meist weiß) wird die einzelne Beobachtung informativ.

$\rightarrow$ Hohe Wahrscheinlichkeit, dass die nächsten Murmeln ebenfalls schwarz sind

**Hierarchische Struktur**

- Informationen werden über Murmelbeutel hinweg auf höherer Ebene geteilt.
- Beobachtungen aus früheren Murmelbeuteln formen starke Priors.
- Diese Priors beeinflussen Vorhersagen über neue Murmelbeutel.


:::: larger
::: callout-important
### Zielsetzung

Entwicklung eines Bayesianisches-Modell, das menschliche Schlussfolgerung über Farbverteilungen zwischen Murmelbeuteln rekonstruiert kann (_reverse-engineering_).

:::
::::

## Formalisierung des Problems {auto-animate="true"}

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

## Formalisierung des Problems {auto-animate="true"}

::: {.notes}

- Level 1 represents the raw data
- For any specific bag $i$, we draw a total of $n$ marbles.
- We observe that $y$ of them are black.
- This gives us our dataset $d_i$ for that specific bag.
- At this level, we are just counting what we see.

:::

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

:::::: {.columns .valign}
::: {.column .column width="50%"}
**Level 1 – Daten**
:::

::: {.column .column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::

::: {.column .column width="15%"}
![][1]
:::
::::::

## Formalisierung des Problems {auto-animate="true"}

::: {.notes}

- Welcher Prozess hat diese Daten generiert?
- n-fache unabhängige Ziehung von einer binären Variablen folgt der Binomialverteilung
- $\theta_i$: Wahrscheinlichkeit in Beutel $i$ eine schwarze Kugel zu ziehen
- Kann über Beutel hinweg unterscheiden (z.B. für einen Beutel fast null und für andere fast eins)

:::

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

::::: columns
::: {.column width="65%"}
**Level 1 – Daten**
:::

::: {.column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 2 – Beutelspezifische Verteilung**
:::

::: {.column width="35%"}
$y_i ~ \big| ~ n_i \sim \text{Binom}(\theta_i)$
:::
:::::

## Formalisierung des Problems {auto-animate="true"}

::: {.notes}

- Welcher Prozess hat diese Daten generiert?
- n-fache unabhängige Ziehung von einer binären Variablen folgt der Binomialverteilung
- $\theta_i$: Wahrscheinlichkeit in Beutel $i$ eine schwarze Kugel zu ziehen
- Kann über Beutel hinweg unterscheiden (z.B. für einen Beutel fast null und für andere fast eins)

:::

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

::::: columns
::: {.column width="65%"}
**Level 1 – Daten**
:::

::: {.column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 2 – Beutelspezifische Verteilung**

-  $\theta_i$: Wahrscheinlichkeit, eine schwarze Murmel aus Beutel $i$ zu ziehen
-  Verschiedene Beutel können unterschiedliche Wahrscheinlichkeiten $\theta_i$ haben

:::

::: {.column width="35%"}
$y_i ~ \big| ~ n_i \sim \text{Binom}(\theta_i)$

<br>

![](/img/binomial_distribution_tour.gif)

:::
:::::

## Formalisierung des Problems {auto-animate="true"}

::: {.notes}

- Wenn wir bei Level 2 stoppen können Informationen über Beutel hinweg nicht ausgetauscht werden 
- Annahme, dass individuelles $\theta_i$ aus einer gemeinsamen Populationsverteilung gezogen werden, die die allgemeine Beschaffenheit von Beutel repräsentiert 
- Wichtigstes Level!
- Erklärung der Parameter
:::

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

:::::: {.columns .valign}
::: {.column .column width="65%"}
**Level 1 – Daten**
:::

::: {.column .column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::

::::::

:::::: {.columns .valign}
::: {.column .column width="65%"}
**Level 2 – Beutelspezifische Verteilung**
:::

::: {.column .column width="35%"}
$y_i ~ \big| ~ n_i \sim \text{Binom}(\theta_i)$
:::

::::::

:::::: columns
::: {.column width="65%"}
**Level 3 – Allgemeines Wissen über Murmelbeutel**
:::

::: {.column width="35%"}
$\theta_i \sim \text{Beta}(\alpha, \beta)$
:::

::::::

## Formalisierung des Problems {auto-animate="true"}

::: {.notes}

- Reparametrisierung ermöglicht leichtere Interpretationd der Parameter
- Mittelwertsparameter $\frac{\alpha}{\alpha+\beta}$ beschreibt die mittlere Wahrscheinlichkeit einen schwarze Murmel zu ziehen über die Population an Beuteln hinweg
- Präzision ($\alpha + \beta$) beschreibt wie homogen oder heterogen die Farbverteilung innerhalb der Beutel ist
  - Hohe Werte $\rightarrrow$ gemischte Farben in Beutel (Verteilung streut nur gering um ihren Mittelwert, kleine Varianz)
  - Niedrige Werte $\rightarrow$ homogene Farben innerhalb von Beutel (alle-weiß / alle-schwarz)

:::

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

::::: columns
::: {.column width="65%"}
**Level 1 – Daten**
:::

::: {.column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 2 – Beutelspezifische Verteilung**
:::

::: {.column width="35%"}
$y_i ~ \big| ~ n_i \sim \text{Binom}(\theta_i)$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 3 – Allgemeines Wissen über Murmelbeutel**

- Mittlere Wahrscheinlichkeit ($\frac{\alpha}{\alpha + \beta}$) eine schwarze Kugel zu ziehen über die Population von Beuteln 
- Precision-Paramter ($\alpha + \beta$) beschreibt die Konzentration der Wahrscheinlichkeitsmasse um den Mittelwert (invers zur Varianz)
:::

::: {.column width="35%"}
$\theta_i \sim \text{Beta}(\alpha, \beta)$

![](/img/beta_distribution_tour.gif)

:::
:::::


## Formalisierung des Problems {auto-animate="true"}

::: {.notes}

- Diese Parameter brauchen den bayesianischen Approach wiederum eigene Prior, sog. Hyperprior
- Nur sehr wenige Grundannahmen, bevor wir Daten gesehen haben
- Uniformer Prior für $\alpha / (\alpha + \beta)$ bedeutet jede mögliche mittlere Wahrscheinlichkeit eine schwarze Kugel zu ziehen ist gleich wahrshceinlich
- Exponentieller Prior über $\alpha + \beta$ $\rightarrow$ Bias in Richtung kleinere Werte des Parameters (Tendenz für homogene Beutel)

:::

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

::::: columns
::: {.column width="65%"}
**Level 1 – Daten**
:::

::: {.column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 2 – Beutelspezifische Verteilung**
:::

::: {.column width="35%"}
$y_i ~ \big| ~ n_i \sim \text{Binom}(\theta_i)$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 3 – Allgemeines Wissen über Murmelbeutel**
:::

::: {.column width="35%"}
$\theta_i \sim \text{Beta}(\alpha, \beta)$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 4 – Hyperparameter**
:::

::: {.column width="35%"}
$\frac{\alpha}{\alpha + \beta} \sim \text{Unif}(0, 1)$

$\alpha + \beta \sim \text{Exp}(1)$
:::
:::::

## Formalisierung des Problems {auto-animate="true"}

::: {.notes}

- Finally, to complete the Bayesian model, we need a prior for these top-level parameters.
- We want to start with as few assumptions as possible.
- We place a Uniform Prior on the mean—meaning prior to seeing any data, we assume the average color could be anything.
- We place an Exponential Prior on the precision. This acts as a "soft" bias towards simpler, lower-count distributions, unless the data strongly tells us otherwise.
- This completes the hierarchy. We have connected the raw data ($y$) all the way up to the abstract hyperparameters.

:::

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

::::: columns
::: {.column width="65%"}
**Level 1 – Daten**
:::

::: {.column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 2 – Beutelspezifische Verteilung**
:::

::: {.column width="35%"}
$y_i ~ \big| ~ n_i \sim \text{Binom}(\theta_i)$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 3 – Allgemeines Wissen über Murmelbeutel**
:::

::: {.column width="35%"}
$\theta_i \sim \text{Beta}(\alpha, \beta)$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 4 – Hyperparameter**

- Prior der Beta-Verteilung
- Uniformer Prior für ($\frac{\alpha}{\alpha + \beta}$) $\rightarrow$ jede mittlere Wahrscheinlichkeit, eine schwarze Murmel zu ziehen gleich wahrscheinlich
-  Exponentielle Verteilung für ($\alpha + \beta$) $\rightarrow$ kleinere Werte wahrscheinlicher (Tendenz zur Homogenität innerhalb der Beutel) 

:::

::: {.column width="35%"}
$\frac{\alpha}{\alpha + \beta} \sim \text{Unif}(0, 1)$

$\alpha + \beta \sim \text{Exp}(1)$

![](figures/priors.png)
:::
:::::

## Formalisierung des Problems {auto-animate="true"}

Wir ziehen aus $i$ Murmelbeuteln, wobei $y_i$ die Anzahl an gezogenen schwarzen Murmel und $n_i$ die ingesamt gezogenen Murmeln repräsentiert.

::::: columns
::: {.column width="65%"}
**Level 1 – Daten**
:::

::: {.column width="35%"}
$d_i:  \big\{y_i, n_i \big\}$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 2 – Beutelspezifische Verteilung**
:::

::: {.column width="35%"}
$y_i ~ \big| ~ n_i \sim \text{Binom}(\theta_i)$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 3 – Allgemeines Wissen über Murmelbeutel**
:::

::: {.column width="35%"}
$\theta_i \sim \text{Beta}(\alpha, \beta)$
:::
:::::

::::: columns
::: {.column width="65%"}
**Level 4 – Hyperparameter**
:::

::: {.column width="35%"}
$\frac{\alpha}{\alpha + \beta} \sim \text{Unif}(0, 1)$

$\alpha + \beta \sim \text{Exp}(1)$
:::
:::::

## Posteriore Inferenz {auto-animate="true"}

::: {.notes}
- Bayes-Formel wird für HBMs noch etwas komplizierter, weil wir nicht nur Likelihood und Prior haben die proportional zum Posterior sind
- Stattdessen mittelt dieses Integral über alle möglichen Werte von ($\alpha$) und ($\beta$), gewichtet danach, wie wahrscheinlich sie gegeben die Daten ($d_1 \dots d_n$) sind
:::


::: {.fragment}

Anwendung von Bayes-Formel bei hierarchischen Modellen [@gelman2014]

$$
\begin{gathered}
\overbrace{P(\theta, \alpha, \beta ~ | ~ y)}^{\text{Posterior}} \propto 
\underbrace{P(\alpha, \beta)}_{\text{Hyperprior}} \overbrace{P(\theta ~ | ~ \alpha, \beta)}^{\text{Conditional Prior}} \underbrace{P(y ~ | ~ \theta, \alpha, \beta)}_{\text{Likelihood}}
\end{gathered}
$$

:::

::: {.fragment}

Posterior Inferenz bezüglich  $\theta_i$ durch Integration über $\alpha$ und $\beta$ [@griffiths2024]

$$
\begin{align*}
P(\theta_i ~ | ~ d_1, \dots, d_n) =   
\iint P(\theta_i ~ | ~ \alpha, \beta, d_i) P(\alpha, \beta ~ | ~ d_1, \dots, d_n) \,d\alpha  \,d \beta 
\end{align*}
$$

:::

## Anwendung des Modells auf das Murmelbeispiel {.nocenter}

::: panel-tabset
### Szenario 1

![][10]

### Szenario 2

![][11]
:::

## Zwischenfazit {auto-animate="true"}

Das Murmelbeispiel zeigt, dass HBMs gut mit unserer Intuition übereinstimmen, wie hierarchisch strukturierte Daten genutzt werden können, um Generalisierungen (_overhypotheses_) zu bilden.

<br>

_Wieso ist das wichtig?_

Diese Abstrakte Wissen ermöglicht schnelles Lernen aus nur wenigen Daten sowie _One-Shot-Generalisierung_.


# Anwendung von HBMs to <br> *Lernen von Kategorien* {.dark-theme}


##  {auto-animate="true"}

![][12]

## Einführendes Beispiel {auto-animate="true"}

:::::: columns
::: column

![][12]{fig-align="center" width="60%"}

:::

:::: column

Szenario: Eine Mutter zeigt auf einen unbekannten Gegenstand und sagt zu ihrem Kind, dass dies ein _Stift_ sei.

::::
::::::

::: {.fragment}
**Frage**

Anhand welcher Merkmale verallgemeinern Kinder das Konzept „Stift“ und erkennen zukünftige Exemplare eines Stifts als solchen an?

:::


::: fragment

- Grundsätzlich könnte das Kind das Wort auf Objekte mit *gleichem Material*, *gleicher Farbe*, *gleicher Textur* oder einfach *Objekten auf der Arbeitsplatte* übertragen.
- Empirisch neigen Kinder jedoch dazu, das neue Wort auf andere Objekte zu übertragen, die die **gleiche Form** haben.
:::


:::: fragment
:::: larger
::: callout-important
### Shape Bias
Die Erwartung, dass Mitglieder einer Kategorie tendenziell eine ähnliche Form haben.
::::
::::
::::

## Adapation des Hierarchischen Beta-Binomialen Modells

::: {.notes}

- Next question becomes how we can use the same modeling ideas from the first example and adapt it model the shape bias of children?

:::


```{r}
#| label: model-changes
#| tbl-cap: Übersicht der Modeladaptionen

data.frame(
  id = c("Hierarchisch Variable", "Daten", "Merkmale", "Merkmalswerte"),
  marble = c("Beutel", "Murmel", "Farbe", "Binär"),
  shape = c("Objektkategorie", "Exemplare", "Form, Farbe, Textur, Größe, etc.", "Kategorisch")
) |>
  kbl(
    align = "c",
    col.names = NULL
  ) |> 
  add_header_above(c(" " = 1, "Murmel-Beispiel" = 1, "Form-Beispiel")) |>
  column_spec(2:3, background = "#f0f0f0") |> 
  kable_styling(
    position = "center"
  )


```

Nach @kemp2007:
- **Level 1**: Binäre Beobachtungen $\rightarrow$ Kategoriale Beobachtungen
- **Level 2**: Binomialverteilung $\rightarrow$ Multinomialverteilung
- **Level 3**: Beta-Prior $\rightarrow$ Dirichlet-Prior
- **Level 4**: Hyperprior wie zuvor

- Kopie von Level 2–4 **für jede Merkmalsdimension** (Farbe, Form, Textur, Größe)

## Model Adaption

::: {layout-ncol=2}

- Das Modell schließt, dass Kategorien in der **Form konsistent** sind (geringe Varianz), aber in der **Farbe variabel** (hohe Varianz)
- Diese gelernte Struktur erzeugt eine **starke Prior-Erwartung**, dass jede neue Kategorie ebenfalls in der Form homogen ist
-  Modell **schnelle Generalisierung** neuer Labels basierend auf **Formähnlichkeit** und ignoriert effektiv Unterschiede in der Farbe $\rightarrow$ Shape Bias

![][13]

:::


## Anwendung auf die Nomen-Generalisisation


```{r smith2002-data}

smith2002 <- tribble(
  ~feature, ~c11, ~c21, ~c12, ~c22, ~c13, ~c23, ~c14, ~c24,
  "Kategorie", 1, 1, 2, 2, 3, 3, 4, 4,
  "Form",    1, 1, 2, 2, 3, 3, 4, 4,
  "Textur",  1, 2, 3, 4, 5, 6, 7, 8,
  "Farbe",    1, 2, 3, 4, 5, 6, 7, 8,
  "Größe",     1, 2, 1, 2, 1, 2, 1, 2
)

# Create table with spanners
table_training <- smith2002 |>
  kbl(
    align = "c",
    col.names = NULL,
    #caption = "Smith (2002) Table",
  ) |>
  add_header_above(c(" " = 1, "1" = 2, "2" = 2, "3" = 2, "4" = 2)) |>
  column_spec(2:3, background = "#f0f0f0") |> 
  column_spec(4:5, background = "#ffffff") |>  
  column_spec(6:7, background = "#f0f0f0") |>  
  column_spec(8:9, background = "#ffffff") |>  
  kable_styling(
    full_width = FALSE,
    position = "center"
  ) 

smith2002_testing <- tribble(
  ~test, ~t1, ~t2, ~t3,
  5, NA, NA, NA,
  5, 5, 6, 6, 
  9, 10, 9, 10, 
  9, 10, 10, 9, 
  1, 1, 1, 1
)

table_testing <- smith2002_testing |>
  mutate(feature = c("Kategorie", "Form", "Textur", "Farbe", "Größe"), .before = 1) |>
  kbl(
    align = "c",
    col.names = NULL,
    #caption = "Testing",
    table.attr = "style='width:50%;'"
  ) |> 
  add_header_above(c(" " = 1, 
                     "'Dax'" = 1, "Objekt 1" = 1, "Objekt 2" = 1, "Objekt 3" = 1)) |>
  column_spec(2, background = "#f0f0f0") |> 
  kable_styling(
    full_width = FALSE,
    position = "center"
  ) 

```

::::::::: columns
::::: column
::: fragment
```{r}
#| label: tbl-training
#| tbl-cap: Training Data 
table_training
```
:::

::: fragment
-   Zwei Exemplare pro Kategorie (Spalten)
-   Verschiedene Merkmalsdimensionen (Form, Textur, Farbe, Größe)
-   Paare von Objekten derselben Kategorie teilen die gleiche Form

:::
:::::

::::: column
::: fragment
```{r}
#| label: tbl-testing
#| tbl-cap: Testing Data 
table_testing
```
:::

::: fragment
Nach dem Training stoßen Kinder (und das Modell) auf ein neues Objekt mit dem neuen Nomen *„dax“*.

**Aufgabe:** Welches der drei Kandidatenobjekte mit unbekannter Kategorie ist am wahrscheinlichsten ein *dax*?

:::
:::::
:::::::::

Basierend auf Daten von @smith2002

## Results of Noun Generalization Task

::::: columns
::: column
- 19 Monate alte Kinder, die das strukturierte Training erhielten, wählen das Objekt mit **gleicher Form** $\rightarrow$ Shape Bias
- Untrainierte 19 Monate alte Kinder wählen **zufällig**
- HBM wist gleichen **Präferenzmuster wie die trainierten Kinder** auf

:::

::: column
![][14]
:::
:::::

## Take Home Message

Hierarchische Bayes-Modelle zeigen, dass **induktive Biases** (wie der Shape Bias) nicht angeboren sein müssen, sondern aus der statistischen **Struktur** der Umwelt gelernt werden können.

## Literatur

  [1]: /figures/single/marbles_1_phi_0.8.png
  [2]: /figures/single/marbles_2_phi_0.8.png
  [3]: /figures/single/marbles_3_phi_0.8.png
  [4]: /figures/single/marbles_4_phi_0.8.png
  [5]: /figures/single/marbles_5_phi_0.8.png
  [6]: /figures/single/marbles_6_phi_0.8.png
  [7]: /figures/single/marbles_7_phi_0.8.png
  [8]: /figures/single/marbles_8_phi_0.8.png
  [9]: /figures/priors.png
  [10]: /figures/combined_phi_0.8.svg
  [11]: /figures/combined_phi_20.svg
  [12]: /img/pen.png
  [13]: /img/hierarchy_category-learning.png
  [14]: /img/dax.png